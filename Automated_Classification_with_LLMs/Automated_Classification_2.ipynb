{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "Created by [Erik Fredner](https://fredner.org) for the 2024 Text Analysis Pedagogy Institute. Revised and expanded by Zhuo Chen under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "\n",
    "For questions/comments/improvements, email zhuo.chen@ithaka.org or nathan.kelber@ithaka.org<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# Automated Text Classification Using LLMs 2\n",
    "\n",
    "**Description:** This notebook describes:\n",
    "\n",
    "* What is F-score\n",
    "* How to evaluate the performance of the LLM classification outputs using F-score\n",
    "* How to create gold standard data for the evaluation\n",
    "\n",
    "**Use Case:** For Learners and Researchers\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Completion Time:** 90 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python Basics Series ([Start Python Basics 1](../Python-basics/python-basics-1.ipynb))\n",
    "* Python Intermediate Series ([Start Python Basics 1](../Python-basics/python-basics-1.ipynb))\n",
    "* Introduction to LLMs ([Start Intro to LLMs 1](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/March+20+2024_+How+ChatGPT+works+(Session+1).pdf))\n",
    "* Automated Classificaton using LLMs 1 ([Review Automated Classificaton using LLMs 1](../Automated-classification/automated-classification-1.ipynb))\n",
    "\n",
    "**Knowledge Recommended:** Experience with LLM chatbot (e.g. ChatGPT)\n",
    "\n",
    "**Data Format:** JSON\n",
    "\n",
    "**Libraries Used:** openai, dotenv, tiktoken, JSON\n",
    "\n",
    "**Research Pipeline:**\n",
    "1. Play with LLMs if you have not already.\n",
    "2. Test using a chatbot interface for an LLM (like ChatGPT) to perform relevant classifications for your research.\n",
    "3. Evaluate initial results.\n",
    "4. Learn how to interact with an API through this notebook.\n",
    "5. Modify your initial experiments based on what we cover."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c0555",
   "metadata": {},
   "source": [
    "## Install required Python libraries\n",
    "\n",
    "Let's install the required libraries for this lesson. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff5ae40-a08c-4614-be41-660fc2945ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### install the required libraries\n",
    "%pip install --upgrade openai tiktoken python-dotenv # for interaction with the OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5480e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries ###\n",
    "\n",
    "from openai import OpenAI \n",
    "import pandas as pd\n",
    "import random\n",
    "from dotenv import load_dotenv # to load API key\n",
    "import random\n",
    "import json \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dedd148",
   "metadata": {},
   "source": [
    "## Download the sample data\n",
    "Let's download the natural gas sentiment data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019627d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the sample dataset\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Check if a data folder exists. If not, create it.\n",
    "data_folder = Path('./data/')\n",
    "data_folder.mkdir(exist_ok=True)\n",
    "\n",
    "url = 'https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/natural_gas_sents.jsonl'\n",
    "file_path = 'data/' + url.rsplit('/')[-1]\n",
    "urllib.request.urlretrieve(url, file_path)\n",
    "print('Sample file ready.')\n",
    "\n",
    "ng_df = pd.read_json('data/natural_gas_sents.jsonl', lines=True)\n",
    "ng_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcaaab1",
   "metadata": {},
   "source": [
    "Now that we know how to use the OpenAI API to do sentiment analysis in an automated way, how do we evaluate the performance of the LLM? \n",
    "# How do you evaluate an LLM's classifications?\n",
    "\n",
    "- How well do the LLM's judgments align with the gold standard?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec4c405",
   "metadata": {},
   "source": [
    "## Using the gold standard\n",
    "We will compare the LLM output classification with the gold standard. Specifically. we will use a statistic **F-score** to evaluate the performance of the LLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b13d8b6",
   "metadata": {},
   "source": [
    "### Review of lesson 1\n",
    "\n",
    "Let's first briefly review what you have learned in lesson 1 about using OpenAI API to classify texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637a8791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the system message\n",
    "system_message = \"\"\"Determine whether the following sentence mentioning natural gas conveys a positive, negative or neutral sentiment. Sentiment: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f163ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() # load the API key\n",
    "client = OpenAI() # load OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f0e258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a chat completion function\n",
    "def make_completion(user_message, client=OpenAI(), model='gpt-4o-2024-08-06', print_message=False):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "        ],\n",
    "    )\n",
    "    if print_message:\n",
    "        print(f\"System message: {system_message}\\n{'-' * 80}\") # print system message\n",
    "        print(f\"User message: {user_message}\\n{'-' * 80}\") # print user message\n",
    "        print(\n",
    "            f\"Assistant response: {completion.choices[0].message.content}\\n{'*' * 80}\" # get the LLM response\n",
    "        )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bdfbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the first row in the data to test\n",
    "test = make_completion(ng_df.iloc[0]['line_text'], client=OpenAI(), model='gpt-4o-2024-08-06', print_message=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46bac42-038b-43a0-b2d7-56fa5d16535d",
   "metadata": {},
   "source": [
    "Now you know how to classify the sentence in each row into one of three categories --- positive, negative or neutral--- using the OpenAI API. Let's create a column to store the LLM output to facilitate the evaluation in the next section. For demonstration purposes, we will only select three rows from the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b5ff5c-ec74-4efa-8a75-1876e8b223e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 3 rows from the datafrmae\n",
    "columbia = ng_df.loc[ng_df['school']=='Columbia University_Center on Global Energy Policy'].sample(1)\n",
    "mit = ng_df.loc[ng_df['school']=='Massachusetts Institute of Technology_MIT Energy Initiative'].sample(1)\n",
    "stanford = ng_df.loc[ng_df['school']=='Stanford_Natural Gas Initiative'].sample(1)\n",
    "sample_df = pd.concat([columbia, mit, stanford]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98de3d03-4fa3-4e13-bcd9-492dbc616b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column storing the LLM output\n",
    "# note that only 3 rows are selected for demonstration\n",
    "sample_df['LLM_output'] = sample_df['line_text'].apply(make_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34341640-54e4-4533-b590-511b00319820",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d70c59-146b-40b1-ba14-77c6be79c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparing LLM classifications to human classifications\n",
    "sample_df['LLM_output'] = sample_df['LLM_output'].str.lower()\n",
    "sample_df['LLM_Gold_agree'] = sample_df.apply(lambda row: row['sentiment'] == row['LLM_output'], axis=1) \n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37203d8-b0a2-4022-9155-b6ebe12da4db",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-09-16T02:39:34.656410Z",
     "iopub.status.busy": "2024-09-16T02:39:34.655880Z",
     "iopub.status.idle": "2024-09-16T02:39:34.663541Z",
     "shell.execute_reply": "2024-09-16T02:39:34.663060Z",
     "shell.execute_reply.started": "2024-09-16T02:39:34.656387Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The most basic way of measuring the performance of the LLM is of course to calculate how often the LLM outputs the same label as the gold standard data. \n",
    "\n",
    "There is, however, a widely used statistic in machine learning for evaluating the performance of a language model on a classification task: the F-score. \n",
    "\n",
    "In the Intro to Language Models series, you have learned the concept of the F-score. Let's do a quick review of this concept. \n",
    "\n",
    "## The F-Score\n",
    "Two important concepts related to the F-score are: **precision** and **recall**.\n",
    "\n",
    "### F-score in binary classification\n",
    "In binary classification, the F1-score is calculated based on True Positives (TP), False Positives (FP), and False Negatives (FN), using the following formula:\n",
    "\n",
    "#### Precision\n",
    "$Precision = \\frac{TP}{TP + FP}$\n",
    "\n",
    "**Precision** measures how many of the items the model identified as `True` were really `True` according to the gold standard data (i.e. true positives). That is, it answers the question: among all the items classified as True, how many are actually True? \n",
    "\n",
    "#### Recall\n",
    "$Recall = \\frac{TP}{TP + FN}$\n",
    "\n",
    "**Recall** measures how many of the True values in the gold standard are correctly labeled as True by the model. That is, it answers the question:  among all the items that are actually True, how many are identified as True by the model? \n",
    "\n",
    "#### F-score (aka F1)\n",
    "\n",
    "The F score is the **harmonic mean** of precision and recall.\n",
    "\n",
    "$F_{1}= \\frac{2}{\\frac{1}{Precision}+\\frac{1}{Recall}}= 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$\n",
    "\n",
    "#### Using Sklearn to calculate F-score\n",
    "\n",
    "There is an easy way to calculate this score using [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f9f71c-4301-48ea-9f0b-750cb2be6c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the f1_score function from sklearn\n",
    "from sklearn.metrics import f1_score # to evaluate the performance of LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71709276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a toy example for F-score\n",
    "y_true = [0, 1, 0, 0, 1, 1] # gold standard\n",
    "y_pred = [0, 1, 1, 0, 1, 1] # model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d5d80-4323-4013-a24c-b584cbf30fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d04ce5-87c8-4196-bb8b-76da000e0de7",
   "metadata": {},
   "source": [
    "#### How to interpret F-score\n",
    "\n",
    "* F1-Score = 1:\n",
    "\n",
    "This means both precision and recall are perfect (100%). The model correctly identified all positive instances and made no false predictions.\n",
    "\n",
    "* F1-Score = 0:\n",
    "\n",
    "This means either precision or recall (or both) is 0. The model is performing poorly, either not identifying any positives or making only incorrect predictions.\n",
    "\n",
    "* Intermediate Values (between 0 and 1):\n",
    "\n",
    "The F1-score balances between precision and recall, so an intermediate value indicates a trade-off:\n",
    "\n",
    "If the score is closer to 1, the model has reasonably good precision and recall.\n",
    "\n",
    "If the score is closer to 0, either precision, recall, or both are poor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e40743b-c51b-4c95-9e43-c804780d1831",
   "metadata": {},
   "source": [
    "### F-score in multi-class classification\n",
    "In our example project, we are using OpenAI API to classify sentences containing 'natural gas' into one of three categories: positive, negative or neutral. This is a multi-class classification task. F-score used to evaluate the performance of a language model in a multi-class classification task comes in three variations. They have different use cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93927dff-749e-4755-bf75-6928b4e7e4ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Macro F-score\n",
    "\n",
    "**Macro F1** calculates the F1-score independently for each class, and then takes the average of the F1-scores across all classes.\n",
    "It treats all classes equally, regardless of their frequency. \n",
    "\n",
    "An example use case: \n",
    "\n",
    "Imagine you're building a model to classify news articles into one of several categories: Politics, Sports, Technology, and Health. The dataset has different numbers of articles in each category, but the stakeholders have expressed that all categories are equally important to them, regardless of how frequently they appear in the dataset. They want the model to perform equally well across all categories.\n",
    "\n",
    "$Macro~F1 = \\frac{1}{N}\\sum_{i=1}^{N}F1_{i}$"
   ]
  },
  {
   "cell_type": "raw",
   "id": "899350f4-a193-4264-b1cc-7616755f3c3a",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/markdown",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "|Class     |Precision|Recall|F1-Score|\n",
    "|Politics  |  0.90   | 0.95 |  0.924 |\n",
    "|Sports    |  0.85   | 0.80 |  0.824 |\n",
    "|Technology|  0.70   | 0.65 |  0.674 |\n",
    "|Health    |  0.60   | 0.70 |  0.646 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a32ced3-7949-4969-b924-5150bfa9cccc",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-09-19T02:59:57.604774Z",
     "iopub.status.busy": "2024-09-19T02:59:57.604482Z",
     "iopub.status.idle": "2024-09-19T02:59:57.608561Z",
     "shell.execute_reply": "2024-09-19T02:59:57.608048Z",
     "shell.execute_reply.started": "2024-09-19T02:59:57.604755Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Suppose there are 500 news articles in Politics, Sports and Technology in your dataset but only 100 news articles in Health. If the performance of the language model on all categories are equally important to you, then you can use the **Macro-F1** to evaluate the model. \n",
    "\n",
    "In this example use case, \n",
    "\n",
    "$Macro~F1 = \\frac{0.924 + 0.824 + 0.674 + 0.646}{4} = 0.767$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf816c-f00e-4a3c-bbeb-93babcd5ca7d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Micro F-score\n",
    "**Micro F1** is calculated by summing up the **True Positives (TP)**, **False Positives (FP)**, and **False Negatives (FN)** across all classes and then computes **precision** and **recall** from the aggregated values. \n",
    "\n",
    "$Micro~Precision = \\frac{\\sum TP}{\\sum TP + \\sum FP}$\n",
    "\n",
    "$Micro~Recall = \\frac{\\sum TP}{\\sum TP + \\sum FN}$\n",
    "\n",
    "$Micro~F1 = 2 * \\frac{Micro~Precision~*~Micro~Recall}{Micro~Precision~+~Micro~Recall }$\n",
    "\n",
    "This means  that every individual instance, no matter which class it comes from, is treated equally in terms of the precision and recall. \n",
    "\n",
    "An example use case:\n",
    "\n",
    "Suppose you're building a model to classify medical images into multiple categories such as **Benign Tumor**, **Malignant Tumor**, **Inflammation**, and **No Abnormality**. The dataset is heavily imbalanced: most images are classified as **No Abnormality**, while the other categories (e.g., **Malignant Tumor**) are rare but extremely important.\n",
    "\n",
    "The stakeholders are more interested in correctly classifying all individual cases (especially reducing false positives and false negatives) rather than evaluating each class independently. Therefore, you want the model to focus on the overall performance across all instances, not the performance per class. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "581cc143-9b95-47d2-9355-80a817fd21f2",
   "metadata": {},
   "source": [
    "|Class          | True Positives (TP) | False Positives (FP) | False Negatives (FN)|\n",
    "|Benign Tumor   |        80           |          20          |         10          |\n",
    "|Malignant Tumor|        5            |          15          |         5           |\n",
    "|Inflammation   |        25           |          5           |         10          |\n",
    "|No Abnormality |        800          |          50          |         20          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f9ca7",
   "metadata": {},
   "source": [
    "$Precision = \\frac{80+5+25+800}{80+5+25+800+20+15+5+50}=0.91$\n",
    "\n",
    "$Recall=\\frac{80+5+25+800}{80+5+25+800+10+5+10+20}=0.953$ \n",
    "\n",
    "$Micro~F1= 2 * \\frac{0.91 * 0.953}{0.91 + 0.953} = 0.931$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5ab81c-eab8-44e4-b880-83ec3eef4732",
   "metadata": {},
   "source": [
    "#### Weighted-average-F1-Score\n",
    "**Weighted F1** is calculated by taking the F1-score of each class and weights it by the proportion of the true instances for that class. The number of true instances in a class is called the **support** of that class. \n",
    "\n",
    "$Weighted~F1=\\frac{\\sum_{i=1}^{N}(F1_{i} \\times Support_{i})}{\\sum_{i=1}^{n}Support_{i}}$\n",
    "\n",
    "An example use case: \n",
    "Suppose you are building a classification model to detect different types of defects --- cracks, dents, misalignment, discoloration --- of products on an assembly line. The dataset is imbalanced because some types of defects are much more common than others. The stakeholders are interested in identifying as many products defects as possible across all classes. Therefore, you would not want to penalize the model's performance on the more rare classes. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "74526463-b9b0-439b-918e-9e038580b187",
   "metadata": {},
   "source": [
    "              |      Predicted     |        Cracks      |      Dents          |    Misalignment    | Discoloration|\n",
    "|   True      |                    |                    |                     |                    |              |\n",
    "|  Cracks     |                    |         60         |        5            |         3          |       2      |\n",
    "|   Dents     |                    |          4         |        15           |         0          |       1      |\n",
    "| Misalignment|                    |          1         |        0            |         4          |       0      |\n",
    "|Discoloration|                    |          0         |        2            |         1          |       2      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e2f590-9b3e-40c5-942b-c24298534fb6",
   "metadata": {},
   "source": [
    "* Cracks\n",
    "\n",
    "$Precision = \\frac{60}{60+5}\\approx 0.9231$\n",
    "\n",
    "$Recall = \\frac{60}{60+10} \\approx 0.8571$\n",
    "\n",
    "$F1 = 2 * \\frac{0.9231 * 0.8571}{0.9231 + 0.8571} = 0.8889$\n",
    "\n",
    "$weight = \\frac{60}{100}=0.6$\n",
    "\n",
    "* Dents \n",
    "\n",
    "$Precision = \\frac{15}{15 + 7} \\approx 0.6818$\n",
    "\n",
    "$Recall = \\frac{15}{15 + 5} = 0.75$\n",
    "\n",
    "$F1 = 2 * \\frac{0.6818 * 0.75}{0.6818 + 0.75} \\approx 0.7143$\n",
    "\n",
    "$weight = \\frac{15}{100} = 0.15$\n",
    "\n",
    "* Misalignment\n",
    "\n",
    "$Precision = \\frac{4}{4+4}=0.5$\n",
    "\n",
    "$Recall = \\frac{4}{4+1}=0.8$\n",
    "\n",
    "$F1 = 2 * \\frac{0.5 * 0.8}{0.5 + 0.8} \\approx 0.6154$\n",
    "\n",
    "$weight = \\frac{5}{100}=0.05$\n",
    "\n",
    "* Discoloration\n",
    "\n",
    "$Precision = \\frac{2}{2+2}=0.5$\n",
    "\n",
    "$Recall = \\frac{2}{2+1}\\approx 0.6667$\n",
    "\n",
    "$F1 = 2 * \\frac{0.5 * 0.6667}{0.5 + 0.6667} \\approx 0.5714$\n",
    "\n",
    "$weight = \\frac{5}{100}=0.05$\n",
    "\n",
    "$Weighted~F1 = (0.8889\\times 0.6) + (0.7143 \\times 0.15) + (0.6154 \\times 0.05) + (0.5714 \\times 0.05)= 0.8244$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd92c7ff-dda3-4783-9593-a84a58eb381e",
   "metadata": {},
   "source": [
    "## Back to our example\n",
    "In our example study, the authors are interested in the all of the classes equally. Therefore, we will use the **Macro F1** score. \n",
    "\n",
    "### Use sklearn to calculate the F1 score\n",
    "In the above, you've seen how to use the `f1_score` to evaluate the performance of a model on a toy example of binary classification. The `f1_score` method in `sklearn` has a parameter `average` whose value can be set to `binary`, `micro`, `macro`, `weighted`. The value you give to this parameter determines which type of F1 score you would like to use to evaluate the performance of your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1194044a-a0d2-4ee0-b3b2-9e08f765dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "### for demonstration purposes, we will use the sample_df with 3 samples\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af589aa-f193-47c0-ba01-0c4643fe2c98",
   "metadata": {},
   "source": [
    "We will first turn the sentiment label to numbers for computers to understand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4a23de-a30d-4f9f-abc2-e6c166a8180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the categories to numbers\n",
    "# 0 is neutral, 1 is positive, 2 is negative\n",
    "sample_df['sentiment'] = sample_df['sentiment'].apply(lambda x: 1 if x=='positive' else (0 if x=='neutral' else 2))\n",
    "sample_df['LLM_output'] = sample_df['LLM_output'].apply(lambda x: 1 if x=='positive' else (0 if x=='neutral' else 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98dce4f-ee0d-42f9-94f8-f35548b0352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the resulting df\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9217ee-4b6e-45e0-8153-0df998505a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the macro F1 score of the model's performance \n",
    "y_true = sample_df['sentiment'].tolist()\n",
    "y_pred = sample_df['LLM_output'].tolist()\n",
    "f1_score(y_true,y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfc654a-ae33-4cb8-836c-8b3c53f7b68f",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red; display:inline\">Coding challennge &lt; / &gt; </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817825b6-27b5-4dea-b9fb-569752c83727",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red; display:inline\">Working on your team project! &lt; / &gt; </h3>\n",
    "\n",
    "1. Discuss within your team about which type of F1 score is more appropriate for your project and why. \n",
    "\n",
    "2. Select a subset of your dataset (you are only trying out the pipeline in class) and use what you have learned to do classification. You will also need to do the classification using your own expertise so that you have the gold standard data ready.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c218df-75a1-464e-83a5-8cf079d7ae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip the gold standard column to pretend we don't have them!\n",
    "sample_df = sample_df.drop(columns=['sentiment']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9203e914-7eb4-4d24-ac3c-39dde9da00e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example as to how to call a function to record the gold standard data\n",
    "def get_gold_standard(df):\n",
    "    ### it takes a df of the natural gas data and add a column storing the gold standard data\n",
    "    gold_ans = []\n",
    "    for i in range(len(df)):\n",
    "        text = df.iloc[i].loc['line_text'] \n",
    "        user_input = input(f\"Classify the text\\n{'-' * 80}\\n{text}\\n{'-' * 80} :\")\n",
    "        gold_ans.append(user_input)\n",
    "    df['gold standard'] = gold_ans\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0eaa52-029d-4831-9c2a-57834ef9e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the function to get the gold standard data\n",
    "get_gold_standard(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40ae544-f859-42c4-bfbf-b81d2a804828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's your turn! Try the above to apply to your own data! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e066d465-bbdb-424d-9a6f-d864bf7a97bf",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red; display:inline\">Working on the example Jeopardy dataset! &lt; / &gt; </h3>\n",
    "\n",
    "If you don't have a team project, you can try with the Jeopardy dataset we played with in Lesson 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3283c212-be08-4622-a001-69ac3598c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### download the Jeopardy sample dataset\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Check if a data folder exists. If not, create it.\n",
    "data_folder = Path('./data/')\n",
    "data_folder.mkdir(exist_ok=True)\n",
    "\n",
    "url = 'https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/jeopardy_data.csv'\n",
    "file_path = 'data/' + url.rsplit('/')[-1]\n",
    "urllib.request.urlretrieve(url, file_path)\n",
    "print('Sample file ready.')\n",
    "\n",
    "# Read in the data\n",
    "jeopardy_df = pd.read_csv(file_path)\n",
    "jeopardy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e314b-98ad-4840-a66a-b099e2a9077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a subset from the df and do the classification (you are trying out the pipeline in class)\n",
    "# then, tweak the get_gold_standard() function to apply to the df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4ea7d2-ae42-4330-9ae9-660fd6bab43f",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red; display:inline\">Calculate the F1-score to evaluate the performance of the LLM on your dataset &lt; / &gt; </h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aee946-46c3-4a85-8ed6-458d1ba5ff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sklearn to calculate the F1-score \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea7ef94-ce2e-4f68-a865-f854f4e6f827",
   "metadata": {},
   "source": [
    "# What if I don't have the gold standard data? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abbd270-e6d5-43b2-8167-412502a930ae",
   "metadata": {},
   "source": [
    "## Use LLM's confidence to evaluate the outputs\n",
    "The OpenAI API can output a log probability for the output token. The log probability tells us how confident the LLM is when giving the output token. The closer `logprob` is to 0, the more confident the model is in its response.\n",
    "\n",
    "`logprobs` is an attribute of a `ChatCompletion` object. Therefore, it is quite easy to ask the LLM to output the label together with the `logprob` associated with the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dd124f-fe9f-4e16-98e4-5386656aabde",
   "metadata": {},
   "outputs": [],
   "source": [
    "### how to get the logprob of an output\n",
    "system_message = \"\"\"Determine whether the following sentence mentioning natural gas conveys a positive, negative or neutral sentiment. Sentiment: \"\"\"\n",
    "\n",
    "user_message = \"Easy to assemble. Very sturdy.\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    logprobs=True,  # new\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ],\n",
    ")\n",
    "logprob = completion.choices[0].logprobs.content[0].logprob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0394c1cb-20c4-4046-ac68-58f14e2863b6",
   "metadata": {},
   "source": [
    "We can turn the `logprob` into confidence probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c4ce1d-b3fb-46d7-abd4-c9a68c9b1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert log prob to confidence prob\n",
    "import math\n",
    "confidence = round((math.exp(logprob)), 2)\n",
    "confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df9242-2717-4659-8afd-76d64a9e3d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## classify the data using LLM and output label and confidence prob into two columns\n",
    "\n",
    "# get a small subset\n",
    "example = ng_df.sample(3).copy()\n",
    "\n",
    "def make_completion(user_message, client=OpenAI(), model='gpt-4o-2024-08-06', print_message=False):\n",
    "    completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    logprobs=True,  # new\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ],\n",
    "    )\n",
    "    prediction = completion.choices[0].message.content\n",
    "    confidence = round((math.exp(completion.choices[0].logprobs.content[0].logprob)), 2)\n",
    "    return prediction, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9545df2-cda4-4ef2-9738-61eb256dd3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two new columns to store the response and confidence \n",
    "example[['LLM_output', 'confidence']] = example.apply(lambda row: make_completion(row['line_text']), axis=1, result_type='expand')\n",
    "\n",
    "# take a look at the resulting df\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05dbd36-f7e2-4f95-8621-b0af92b911c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T19:53:12.064872Z",
     "iopub.status.busy": "2024-09-21T19:53:12.064588Z",
     "iopub.status.idle": "2024-09-21T19:53:12.072906Z",
     "shell.execute_reply": "2024-09-21T19:53:12.072286Z",
     "shell.execute_reply.started": "2024-09-21T19:53:12.064851Z"
    }
   },
   "source": [
    "<h2 style=\"color:red; display:inline\">Coding challennge &lt; / &gt; </h2>\n",
    "\n",
    "Take your dataframe, create a column for the confidence scores. \n",
    "\n",
    "Determine a threshhold of confidence. Any outputs by the LLM with a confidence score lower than the threshhold are the outputs you will want to take a look at to confirm or disconfirm the judgement by the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eead3d-8b65-4e45-8890-b6de4b4ab610",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a column for the confidence scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d0f2b1-49c0-4031-8fae-62bf5c078fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### sort the df by the confidence scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebb236f-0296-4a34-9165-03eb7b6a0b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### extract all the data with a LLM confidence score lower than your threshhold\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
